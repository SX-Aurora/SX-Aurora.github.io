---
layout: post
title: "Llama2 on the SX-Aurora Vector Engine"
excerpt: Porting and tuning of llama2.c on the sx-aurora VE, performance measurements and comparison.
category: posts
comments: true
tags: [ve, sxaurora, vector, LLM, Llama, AI, artficial intelligence, large language models, performance]
---

## Llama2 on the Vector Engine

The SX-Aurora Vector Engine (VE) is an accelerator for HPC
applications with large memory bandwidth and vector registers with the
length of 16kbits. It is a long vector system that has been conceived
for HPC and supports only vector computation in 32 and 64 bits.

The following slides summarize a few hours of experiments with
porting, tuning and testing the tiny
[llama2.c](https://github.com/karpathy/llama2.c) project which aims at
providing a sigle compact binary for llama based large language models
inference.

With minimal porting effort, which mainly circumvents the compiler's
refusal to vectorize two loops, we reach unexpectedly good performance
with inference in float32 format:

* VE20B Vector Engine: 27 tokens / s
* VE30B Vector Engine: 44 tokens / s


<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQE93wVjSkKonEDzFnH9ezipFOD-sG8sahyesohjGks1pDNe7hRbeqShG5TSX5rAbI7wN4PC45CvSbF/embed?start=false&loop=false&delayms=3000" frameborder="0" width="1058" height="624" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

---

References and links:

* [Wikipedia](https://en.wikipedia.org/wiki/SX-Aurora_TSUBASA)
